% =============================================
% SECTION 5: DATA LOADING & PREPROCESSING (COMPACT)
% =============================================

\section{Data Loading \& Preprocessing}

\subsection{Dataset and DataLoader}

PyTorch uses \texttt{Dataset} (defines how to access samples) and \texttt{DataLoader} (handles batching/shuffling).

\textbf{Custom Dataset:}
\begin{lstlisting}
from torch.utils.data import Dataset, DataLoader

class SimpleDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Usage
dataset = SimpleDataset(X, y)
loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)

for batch_X, batch_y in loader:
    # Training code
    pass
\end{lstlisting}

\textbf{Key DataLoader parameters:}
\begin{itemize}
    \item \texttt{batch\_size}: 32-256 typically
    \item \texttt{shuffle}: True for training, False for eval
    \item \texttt{num\_workers}: 2-4 for parallel loading
    \item \texttt{pin\_memory}: True if using GPU
\end{itemize}

\subsection{Common Dataset Patterns}

\textbf{Time Series Dataset:}
\begin{lstlisting}
class TimeSeriesDataset(Dataset):
    def __init__(self, data, window_size, forecast_horizon):
        self.data = torch.FloatTensor(data)
        self.window = window_size
        self.horizon = forecast_horizon
    
    def __len__(self):
        return len(self.data) - self.window - self.horizon + 1
    
    def __getitem__(self, idx):
        x = self.data[idx:idx+self.window]
        y = self.data[idx+self.window:idx+self.window+self.horizon]
        return x, y
\end{lstlisting}

\textbf{Loading from files:}
\begin{lstlisting}
class FileDataset(Dataset):
    def __init__(self, file_list, transform=None):
        self.files = file_list
        self.transform = transform
    
    def __len__(self):
        return len(self.files)
    
    def __getitem__(self, idx):
        data = np.load(self.files[idx])  # Load on-demand
        if self.transform:
            data = self.transform(data)
        return torch.FloatTensor(data)
\end{lstlisting}

\subsection{Normalization}

\begin{warningbox}[Always Use Training Statistics]
Compute mean/std from training data only, then apply to val/test.
\end{warningbox}

\begin{lstlisting}
class StandardScaler:
    def __init__(self, data):
        self.mean = data.mean(dim=0)
        self.std = data.std(dim=0)
    
    def transform(self, data):
        return (data - self.mean) / (self.std + 1e-8)

# Usage
scaler = StandardScaler(X_train)
X_train_norm = scaler.transform(X_train)
X_test_norm = scaler.transform(X_test)  # Use training stats!
\end{lstlisting}

\subsection{Train/Val/Test Split}

\begin{lstlisting}
from torch.utils.data import random_split

train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_ds, val_ds, test_ds = random_split(dataset, 
                                          [train_size, val_size, test_size])
\end{lstlisting}

\subsection{Exercises}

\begin{exercise}[5.1: Custom Dataset - $\bigstar\bigstar$]
Create a dataset for synthetic regression data, implement \texttt{\_\_len\_\_} and \texttt{\_\_getitem\_\_}, create a DataLoader, and iterate through batches.
\end{exercise}

\begin{exercise}[5.2: Time Series Dataset - $\bigstar\bigstar\bigstar$]
Implement a windowed time series dataset for predicting future values from past observations.
\end{exercise}

\begin{exercise}[5.3: Normalization Impact - $\bigstar\bigstar\bigstar$]
Train a model with and without normalization. Compare convergence speed and final performance.
\end{exercise}

\clearpage

% =============================================
% SECTION 6: FULLY CONNECTED NETWORKS (COMPACT)
% =============================================

\section{Fully Connected Networks (MLPs)}

\subsection{Core Concepts}

MLPs are universal function approximators. Each layer: $\mathbf{h} = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b})$

\textbf{When to use:} Tabular data, function approximation, small input dimensions

\textbf{Architecture choices:}
\begin{itemize}
    \item \textbf{Depth vs Width:} Start with 2-3 layers; deeper = more parameter-efficient but harder to train
    \item \textbf{Layer sizes:} First hidden layer 1-2× input size, then decrease
\end{itemize}

\subsection{Activation Functions}

\begin{lstlisting}
# ReLU (default choice)
nn.ReLU()

# Leaky ReLU (prevents dead neurons)
nn.LeakyReLU(0.01)

# GELU (for Transformers)
nn.GELU()

# Tanh (for RNNs, zero-centered)
nn.Tanh()
\end{lstlisting}

\textbf{Rule:} Use ReLU unless you have a specific reason not to. Never use Sigmoid in hidden layers (vanishing gradients).

\subsection{Implementation}

\begin{lstlisting}
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super().__init__()
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

# Usage
model = MLP(10, [64, 32], 1)
\end{lstlisting}

\subsection{Initialization}

\begin{lstlisting}
import torch.nn.init as init

def init_weights(m):
    if isinstance(m, nn.Linear):
        init.kaiming_normal_(m.weight, nonlinearity='relu')  # He init for ReLU
        if m.bias is not None:
            init.constant_(m.bias, 0)

model.apply(init_weights)
\end{lstlisting}

\textbf{Rules:}
\begin{itemize}
    \item ReLU networks: He/Kaiming initialization
    \item Tanh/Sigmoid: Xavier/Glorot initialization
\end{itemize}

\subsection{Batch Normalization}

\begin{lstlisting}
class MLPWithBatchNorm(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim):
        super().__init__()
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))  # After Linear
            layers.append(nn.ReLU())
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)
\end{lstlisting}

\textbf{BatchNorm benefits:} Stabilizes training, allows higher learning rates, acts as regularization

\textbf{Important:} Use \texttt{model.train()} and \texttt{model.eval()} properly!

\subsection{Dropout}

\begin{lstlisting}
layers.append(nn.Dropout(0.5))  # After activation
\end{lstlisting}

Typical values: 0.2-0.5. Higher = more regularization.

\subsection{Common Issues}

\begin{itemize}
    \item \textbf{Dead ReLUs:} Use Leaky ReLU or proper initialization
    \item \textbf{Vanishing gradients:} Use ReLU, batch norm, or skip connections
    \item \textbf{Overfitting:} Add dropout, weight decay, or more data
\end{itemize}

\subsection{Exercises}

\begin{exercise}[6.1: Build and Train MLP - $\bigstar\bigstar$]
Create a 3-layer MLP, train on synthetic data ($y = x^2$), plot loss curve.
\end{exercise}

\begin{exercise}[6.2: Activation Comparison - $\bigstar\bigstar\bigstar$]
Train identical MLPs with ReLU, Leaky ReLU, Tanh, and Sigmoid. Compare convergence.
\end{exercise}

\begin{exercise}[6.3: Regularization - $\bigstar\bigstar\bigstar\bigstar$]
Small dataset (200 samples), large model. Add dropout, batch norm, weight decay. Compare overfitting.
\end{exercise}

\clearpage

% =============================================
% SECTION 7: CONVOLUTIONAL NEURAL NETWORKS
% =============================================

\section{Convolutional Neural Networks}

\subsection{Why Convolutions?}

\textbf{Three key properties:}
\begin{enumerate}
    \item \textbf{Local connectivity:} Each neuron connects to small region
    \item \textbf{Parameter sharing:} Same filter across entire input
    \item \textbf{Translation invariance:} Detect features anywhere
\end{enumerate}

\textbf{Use CNNs for:} Images, spatial data, time series (1D conv)

\subsection{Convolution Operation}

\textbf{2D Convolution:}
\[
y[i,j] = \sum_{m}\sum_{n} x[i+m, j+n] \cdot w[m,n] + b
\]

\textbf{Key parameters:}
\begin{itemize}
    \item \textbf{kernel\_size}: Size of filter (3×3, 5×5, etc.)
    \item \textbf{stride}: Step size (1 = slide by 1 pixel)
    \item \textbf{padding}: Add zeros around border
    \item \textbf{dilation}: Spacing between kernel elements
\end{itemize}

\subsection{Output Size Calculation}

\[
\text{output\_size} = \left\lfloor \frac{\text{input\_size} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} \right\rfloor + 1
\]

\textbf{Example:} Input 28×28, kernel 3×3, stride 1, padding 1 → Output 28×28

\begin{pytorchtip}[Same Padding]
To keep spatial size: \texttt{padding = (kernel\_size - 1) // 2}
\end{pytorchtip}

\subsection{Implementation}

\textbf{Basic CNN:}
\begin{lstlisting}
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 28x28 -> 28x28
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 28x28 -> 28x28
        self.pool = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))  # 28x28 -> 14x14
        x = self.pool(torch.relu(self.conv2(x)))  # 14x14 -> 7x7
        x = x.view(x.size(0), -1)  # Flatten
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
\end{lstlisting}

\textbf{Conv2d parameters:}
\begin{lstlisting}
nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)
\end{lstlisting}

\subsection{Pooling}

\textbf{Max Pooling} (most common):
\begin{lstlisting}
nn.MaxPool2d(kernel_size=2, stride=2)  # Downsample by 2
\end{lstlisting}

\textbf{Average Pooling:}
\begin{lstlisting}
nn.AvgPool2d(kernel_size=2, stride=2)
\end{lstlisting}

\textbf{Why pool?}
\begin{itemize}
    \item Reduce spatial dimensions (fewer parameters)
    \item Increase receptive field
    \item Translation invariance
\end{itemize}

\subsection{Receptive Field}

The receptive field is the region of input that affects a neuron.

\textbf{Growth:} Each layer increases receptive field
\begin{itemize}
    \item Layer 1 (3×3 kernel): 3×3 receptive field
    \item Layer 2 (3×3 kernel): 5×5 receptive field
    \item With pooling: grows even faster
\end{itemize}

\subsection{1D and 3D Convolutions}

\textbf{1D Conv for sequences:}
\begin{lstlisting}
nn.Conv1d(in_channels, out_channels, kernel_size)

# Example: time series with 10 features, window size 5
conv1d = nn.Conv1d(10, 32, kernel_size=5)
x = torch.randn(batch, 10, 100)  # (batch, channels, length)
out = conv1d(x)  # (batch, 32, 96)
\end{lstlisting}

\textbf{3D Conv for volumes:}
\begin{lstlisting}
nn.Conv3d(in_channels, out_channels, kernel_size)

# Example: 3D medical image
conv3d = nn.Conv3d(1, 32, kernel_size=3)
x = torch.randn(batch, 1, 64, 64, 64)  # (batch, channels, D, H, W)
out = conv3d(x)
\end{lstlisting}

\subsection{Transposed Convolutions}

For upsampling (e.g., in autoencoders):

\begin{lstlisting}
nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=2)

# Example: 7x7 -> 14x14
upconv = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1)
\end{lstlisting}

\subsection{Complete CNN Example}

\begin{lstlisting}
class VGGStyleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(128 * 8 * 8, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
\end{lstlisting}

\subsection{Exercises}

\begin{exercise}[7.1: Simple CNN - $\bigstar\bigstar$]
Build and train a CNN on MNIST: 2 conv layers + 2 FC layers. Achieve >95\% accuracy.
\end{exercise}

\begin{exercise}[7.2: Output Shape Calculation - $\bigstar\bigstar$]
Given input 32×32, calculate output shapes through: Conv(3×3, stride=1, padding=1) → MaxPool(2×2) → Conv(5×5, stride=2, padding=2).
\end{exercise}

\begin{exercise}[7.3: 1D CNN for Time Series - $\bigstar\bigstar\bigstar$]
Use 1D convolutions to smooth/denoise a noisy sine wave. Compare with moving average.
\end{exercise}

\begin{exercise}[7.4: Build U-Net - $\bigstar\bigstar\bigstar\bigstar$]
Implement a simple U-Net with encoder (downsampling) and decoder (upsampling) using ConvTranspose2d.
\end{exercise}

\clearpage

% =============================================
% SECTION 8: RESIDUAL NETWORKS & SKIP CONNECTIONS
% =============================================

\section{Residual Networks \& Skip Connections}

\subsection{The Degradation Problem}

Deeper networks should perform at least as well as shallow ones (can learn identity mapping). In practice, they perform \textbf{worse} due to optimization difficulties.

\textbf{ResNets solve this} with skip connections.

\subsection{Residual Blocks}

\textbf{Key idea:} Learn residual $F(x) = H(x) - x$ instead of $H(x)$ directly.

\[
y = F(x) + x
\]

If optimal mapping is identity, it's easier to learn $F(x) = 0$ than $H(x) = x$.

\begin{lstlisting}
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
    
    def forward(self, x):
        residual = x  # Save input
        
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        out += residual  # Add skip connection
        out = torch.relu(out)
        return out
\end{lstlisting}

\subsection{Projection Shortcuts}

When dimensions change:

\begin{lstlisting}
class ResidualBlockProjection(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 
                              stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # Projection shortcut if dimensions change
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        residual = self.shortcut(x)
        
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        out += residual
        out = torch.relu(out)
        return out
\end{lstlisting}

\subsection{Building a ResNet}

\begin{lstlisting}
class SimpleResNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool = nn.MaxPool2d(3, stride=2, padding=1)
        
        # Residual blocks
        self.layer1 = self._make_layer(64, 64, 2)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(256, num_classes)
    
    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):
        layers = []
        layers.append(ResidualBlockProjection(in_channels, out_channels, stride))
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.pool(torch.relu(self.bn1(self.conv1(x))))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
\end{lstlisting}

\subsection{Why Skip Connections Work}

\begin{itemize}
    \item \textbf{Gradient flow:} Gradients flow directly through shortcuts
    \item \textbf{Easier optimization:} Easier to learn small adjustments than full transformation
    \item \textbf{Ensemble effect:} Network becomes ensemble of shallower networks
\end{itemize}

\subsection{Exercises}

\begin{exercise}[8.1: Residual Block - $\bigstar\bigstar$]
Implement a basic residual block and verify the skip connection works.
\end{exercise}

\begin{exercise}[8.2: Deep Network Comparison - $\bigstar\bigstar\bigstar$]
Train a 20-layer network with and without skip connections. Compare convergence and final accuracy.
\end{exercise}

\begin{exercise}[8.3: Build Mini-ResNet - $\bigstar\bigstar\bigstar\bigstar$]
Implement a small ResNet with 3 stages. Train on CIFAR-10.
\end{exercise}

\clearpage

% =============================================
% SECTION 9: BATCH & LAYER NORMALIZATION
% =============================================

\section{Normalization Techniques}

\subsection{Batch Normalization}

\textbf{Problem:} Internal covariate shift (distribution of activations changes during training)

\textbf{Solution:} Normalize within each batch

\[
\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta
\]

\begin{lstlisting}
nn.BatchNorm1d(num_features)  # For fully connected
nn.BatchNorm2d(num_features)  # For convolutions
\end{lstlisting}

\textbf{Training vs Eval:}
\begin{lstlisting}
model.train()  # Uses batch statistics
model.eval()   # Uses running mean/std
\end{lstlisting}

\subsection{Layer Normalization}

Normalize across features instead of batch:

\begin{lstlisting}
nn.LayerNorm(normalized_shape)
\end{lstlisting}

\textbf{Use LayerNorm for:}
\begin{itemize}
    \item RNNs and Transformers
    \item Small batch sizes (< 8)
    \item Batch size varies
\end{itemize}

\subsection{Comparison}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Property} & \textbf{BatchNorm} & \textbf{LayerNorm} \\
\midrule
Normalizes over & Batch & Features \\
Batch size sensitive & Yes & No \\
Train/eval difference & Yes & No \\
Common in & CNNs, MLPs & Transformers, RNNs \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Instance Normalization}

For style transfer and GANs:

\begin{lstlisting}
nn.InstanceNorm2d(num_features)  # Normalize each sample independently
\end{lstlisting}

\subsection{Group Normalization}

Middle ground between Layer and Instance:

\begin{lstlisting}
nn.GroupNorm(num_groups, num_channels)
\end{lstlisting}

\subsection{Exercises}

\begin{exercise}[9.1: BatchNorm Impact - $\bigstar\bigstar$]
Train a deep MLP with and without BatchNorm. Compare training stability and speed.
\end{exercise}

\begin{exercise}[9.2: Small Batch Issue - $\bigstar\bigstar\bigstar$]
Train with batch\_size=2. Compare BatchNorm vs LayerNorm performance.
\end{exercise}

\clearpage

% =============================================
% SECTION 10: ATTENTION & TRANSFORMERS
% =============================================

\section{Attention Mechanisms \& Transformers}

\subsection{The Attention Mechanism}

\textbf{Core idea:} Focus on relevant parts of input when producing output.

\textbf{Scaled Dot-Product Attention:}
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

\begin{itemize}
    \item $Q$ (Query): What we're looking for
    \item $K$ (Key): What each position contains
    \item $V$ (Value): Actual information
    \item $d_k$: Key dimension (scaling factor)
\end{itemize}

\subsection{Implementation}

\begin{lstlisting}
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q, K, V: (batch, seq_len, d_k)
    Returns: (batch, seq_len, d_k)
    """
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    attention_weights = torch.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights
\end{lstlisting}

\subsection{Multi-Head Attention}

Run attention in parallel with different learned projections:

\begin{lstlisting}
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # Linear projections and split into heads
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)
        
        # Concatenate heads and project
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.W_o(output)
        
        return output
\end{lstlisting}

\subsection{Positional Encoding}

Transformers have no notion of position. Add positional information:

\begin{lstlisting}
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * 
                            -(torch.log(torch.tensor(10000.0)) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
\end{lstlisting}

\subsection{Transformer Block}

\begin{lstlisting}
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # Multi-head attention with residual
        attn_output = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed-forward with residual
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        
        return x
\end{lstlisting}

\subsection{Complete Transformer Encoder}

\begin{lstlisting}
class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=5000):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])
        
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x, mask=None):
        x = self.embedding(x)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        for layer in self.layers:
            x = layer(x, mask)
        
        return x
\end{lstlisting}

\subsection{Causal Masking}

For autoregressive models (GPT-style):

\begin{lstlisting}
def create_causal_mask(seq_len):
    """Prevent attention to future positions."""
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask == 0  # True where attention is allowed
\end{lstlisting}

\subsection{Using PyTorch's Built-in Transformer}

\begin{lstlisting}
# PyTorch provides nn.Transformer
model = nn.Transformer(
    d_model=512,
    nhead=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    dim_feedforward=2048,
    dropout=0.1
)

# For encoder-only (BERT-style)
encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)
\end{lstlisting}

\subsection{Exercises}

\begin{exercise}[10.1: Scaled Dot-Product Attention - $\bigstar\bigstar$]
Implement attention from scratch. Test on random Q, K, V matrices.
\end{exercise}

\begin{exercise}[10.2: Positional Encoding - $\bigstar\bigstar\bigstar$]
Implement and visualize positional encodings. Plot the encoding for different positions and dimensions.
\end{exercise}

\begin{exercise}[10.3: Transformer for Sequence Classification - $\bigstar\bigstar\bigstar\bigstar$]
Build a transformer encoder for sequence classification. Use on synthetic sequential data.
\end{exercise}

\begin{exercise}[10.4: Attention Visualization - $\bigstar\bigstar\bigstar\bigstar$]
Extract and visualize attention weights. Show which parts of input the model focuses on.
\end{exercise}

\clearpage

% =============================================
% SECTION 11: DEBUGGING & BEST PRACTICES
% =============================================

\section{Debugging \& Best Practices}

\subsection{Gradient Checking}

\begin{lstlisting}
from torch.autograd import gradcheck

# Check if gradients are correct
x = torch.randn(3, 4, requires_grad=True, dtype=torch.float64)
test = gradcheck(lambda t: (t**2).sum(), x, eps=1e-6)
print(f"Gradient check: {test}")
\end{lstlisting}

\subsection{Finding NaN/Inf}

\begin{lstlisting}
# Check for NaN in loss
if torch.isnan(loss):
    print("NaN loss detected!")
    # Check inputs, outputs, gradients
    print(f"Input NaN: {torch.isnan(inputs).any()}")
    print(f"Output NaN: {torch.isnan(outputs).any()}")

# Register hook to catch NaN in backward
def check_nan_hook(module, grad_input, grad_output):
    if any(torch.isnan(g).any() for g in grad_output if g is not None):
        print(f"NaN gradient in {module.__class__.__name__}")

for module in model.modules():
    module.register_backward_hook(check_nan_hook)
\end{lstlisting}

\subsection{Gradient Flow Visualization}

\begin{lstlisting}
def check_gradient_flow(model):
    """Check which layers have vanishing/exploding gradients."""
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            print(f"{name}: {grad_norm:.6f}")
            if grad_norm < 1e-6:
                print(f"  WARNING: Vanishing gradient!")
            if grad_norm > 1e3:
                print(f"  WARNING: Exploding gradient!")
\end{lstlisting}

\subsection{Model Checkpointing}

\begin{lstlisting}
# Save checkpoint
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'best_val_loss': best_val_loss
}
torch.save(checkpoint, 'checkpoint.pth')

# Load checkpoint
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
\end{lstlisting}

\subsection{Random Seed for Reproducibility}

\begin{lstlisting}
import random
import numpy as np

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
\end{lstlisting}

\subsection{Performance Optimization}

\begin{lstlisting}
# Use mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()
    
    with autocast():  # Automatic mixed precision
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()

# Pin memory for faster GPU transfer
loader = DataLoader(dataset, batch_size=32, pin_memory=True)

# Use multiple workers
loader = DataLoader(dataset, batch_size=32, num_workers=4)
\end{lstlisting}

\subsection{Common Debugging Checklist}

\begin{itemize}
    \item ✓ Check data shapes at each layer
    \item ✓ Verify data normalization
    \item ✓ Start with small model and small dataset
    \item ✓ Overfit single batch first
    \item ✓ Check learning rate (too high/low?)
    \item ✓ Monitor train and val loss
    \item ✓ Visualize predictions periodically
    \item ✓ Check for NaN/Inf values
    \item ✓ Verify gradient flow
    \item ✓ Use tensorboard for monitoring
\end{itemize}

\clearpage

% =============================================
% SECTION 12: GRAND CHALLENGES
% =============================================

\section{Grand Challenges: Putting It All Together}

\begin{profnote}
These challenges combine multiple concepts. They're designed to be challenging—expect to debug and iterate!
\end{profnote}

\begin{exercise}[Challenge 1: Logistic Regression from Scratch - $\bigstar\bigstar\bigstar$]
\textbf{Goal:} Understand optimization without high-level APIs.

Build binary classifier without \texttt{nn.Module}:
\begin{enumerate}
    \item Create 2-class dataset (200 points)
    \item Manually initialize $W$ and $b$ with \texttt{requires\_grad=True}
    \item Implement forward pass: $z = Wx + b$, $\hat{y} = \sigma(z)$
    \item Compute BCE loss manually
    \item Update parameters with gradient descent (no optimizer!)
    \item Achieve >90\% accuracy
\end{enumerate}

\textbf{Key learning:} Understanding what PyTorch does under the hood.
\end{exercise}

\begin{exercise}[Challenge 2: Custom Autograd Function - $\bigstar\bigstar\bigstar$]
\textbf{Goal:} Extend PyTorch with custom operations.

Implement custom activation: $f(x) = x \cdot \sin(x)$

\begin{lstlisting}
class SinusoidalActivation(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input * torch.sin(input)
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output * (torch.sin(input) + input * torch.cos(input))
        return grad_input

# Wrap in nn.Module
class MySinActivation(nn.Module):
    def forward(self, x):
        return SinusoidalActivation.apply(x)

# Test with gradcheck
\end{lstlisting}

Use it in a network and verify gradients are correct.
\end{exercise}

\begin{exercise}[Challenge 3: CNN for CIFAR-10 - $\bigstar\bigstar\bigstar\bigstar$]
\textbf{Goal:} Build a production-quality image classifier.

Requirements:
\begin{enumerate}
    \item Build CNN with residual connections
    \item Use data augmentation (random crop, flip, etc.)
    \item Implement learning rate scheduling
    \item Add early stopping
    \item Achieve >80\% test accuracy
    \item Visualize some misclassified examples
\end{enumerate}

\textbf{Bonus:} Use techniques like mixup or cutout for better accuracy.
\end{exercise}

\begin{exercise}[Challenge 4: Transformer for Sequence-to-Sequence - $\bigstar\bigstar\bigstar\bigstar$]
\textbf{Goal:} Build a complete transformer.

Task: Learn to reverse sequences

\begin{enumerate}
    \item Input: sequence of integers [1, 5, 3, 8, 2]
    \item Target: reversed [2, 8, 3, 5, 1]
    \item Build transformer encoder-decoder
    \item Use teacher forcing during training
    \item Implement beam search for inference (optional)
    \item Achieve 100\% accuracy on sequences up to length 20
\end{enumerate}
\end{exercise}

\begin{exercise}[Challenge 5: Function Approximation for PDEs - $\bigstar\bigstar\bigstar\bigstar\bigstar$]
\textbf{Goal:} Apply deep learning to scientific computing.

Approximate solution to heat equation:
\[
\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}
\]

\begin{enumerate}
    \item Generate training data by solving PDE numerically
    \item Build MLP or CNN to learn $(x, t) \to u(x,t)$
    \item Train on multiple initial conditions
    \item Test on unseen initial condition
    \item Visualize solution as heatmap over space and time
    \item Compare with numerical solution
\end{enumerate}

\textbf{Success criterion:} Relative error < 5\% on test set
\end{exercise}

\begin{exercise}[Challenge 6: Complete ML Pipeline - $\bigstar\bigstar\bigstar\bigstar\bigstar$]
\textbf{Goal:} Build production-ready system.

Choose any dataset (e.g., from Kaggle) and build complete pipeline:

\begin{enumerate}
    \item Data loading with custom Dataset
    \item Train/val/test split with stratification
    \item Data normalization and augmentation
    \item Model with appropriate architecture
    \item Training with:
    \begin{itemize}
        \item Learning rate scheduling
        \item Early stopping
        \item Model checkpointing
        \item Gradient clipping if needed
        \item Mixed precision training
    \end{itemize}
    \item Comprehensive evaluation:
    \begin{itemize}
        \item Confusion matrix
        \item ROC curve (if classification)
        \item Error analysis
    \end{itemize}
    \item Save final model and document hyperparameters
\end{enumerate}

\textbf{Bonus:} Deploy with ONNX or TorchScript for inference.
\end{exercise}

\subsection{Final Certification}

If you can complete Challenges 3, 4, and 6, you have strong practical PyTorch skills for scientific computing and research.

You're ready to:
\begin{itemize}
    \item Implement papers from scratch
    \item Build custom architectures for your domain
    \item Debug complex training issues
    \item Apply deep learning to scientific problems
\end{itemize}

Keep learning, keep experimenting, and most importantly—keep building!

\clearpage

\end{document}