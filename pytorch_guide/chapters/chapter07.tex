% =============================================
% SECTION 7: CONVOLUTIONAL NEURAL NETWORKS
% =============================================

\section{Convolutional Neural Networks (CNNs)}

\subsection{Introduction: Why Convolutions?}

MLPs work well for tabular data, but they have serious limitations for spatial data like images:

\textbf{Problems with MLPs for images:}
\begin{itemize}
    \item \textbf{Too many parameters:} A 224×224 RGB image has 150,528 inputs. First hidden layer with 1000 neurons = 150M parameters!
    \item \textbf{No spatial structure:} Treats each pixel independently, ignoring that nearby pixels are related
    \item \textbf{Not translation invariant:} A cat in the top-left corner is different from a cat in the center
    \item \textbf{Can't generalize:} Learning a feature at one position doesn't help recognize it elsewhere
\end{itemize}

\textbf{Convolutions solve these problems through:}
\begin{enumerate}
    \item \textbf{Parameter sharing:} Same kernel applied everywhere (drastically fewer parameters)
    \item \textbf{Local connectivity:} Each neuron only looks at a small region (receptive field)
    \item \textbf{Translation invariance:} Features learned at one position work everywhere
    \item \textbf{Hierarchical learning:} Early layers detect edges, later layers detect complex patterns
\end{enumerate}

\textbf{When to use CNNs:}
\begin{itemize}
    \item Images (most common use case)
    \item Time series (1D convolutions)
    \item Volumetric data (3D medical scans, physical simulations)
    \item Spatial/temporal data with local correlations
\end{itemize}

\textbf{When NOT to use CNNs:}
\begin{itemize}
    \item Tabular data (no spatial structure)
    \item Very small spatial dimensions (convolutions overkill)
    \item When you need long-range dependencies only (use attention/Transformers)
\end{itemize}

\subsection{Theory: How Convolutions Work}

\subsubsection{The Convolution Operation}

A 2D convolution slides a \textbf{kernel} (or \textbf{filter}) over the input, computing dot products:

\[
\text{Output}[i, j] = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \text{Input}[i+m, j+n] \cdot \text{Kernel}[m, n]
\]

\textbf{Example:} 3×3 kernel on 5×5 input

\begin{verbatim}
Input (5x5):              Kernel (3x3):         
1  2  3  4  5             1  0 -1
2  3  4  5  6      *      2  0 -2      
3  4  5  6  7             1  0 -1
4  5  6  7  8
5  6  7  8  9

Output (3x3):
-4  -4  -4
-4  -4  -4
-4  -4  -4
\end{verbatim}

This particular kernel is a vertical edge detector (Sobel filter).

\begin{theorybox}[Key Concepts]
\begin{itemize}
    \item \textbf{Kernel/Filter:} Small matrix of learnable weights (e.g., 3×3, 5×5)
    \item \textbf{Stride:} How many pixels to move the kernel each step (stride=1 → move 1 pixel)
    \item \textbf{Padding:} Add zeros around input to control output size
    \item \textbf{Channels:} Input depth (RGB image has 3 channels)
    \item \textbf{Feature maps:} Output of applying convolution (number = number of kernels)
\end{itemize}
\end{theorybox}

\subsubsection{Parameter Sharing}

A single 3×3 kernel has only 9 parameters, but is applied to every position in the image.

\textbf{MLP for 28×28 image:}
\begin{itemize}
    \item Input: 784 pixels
    \item Hidden layer: 128 neurons
    \item Parameters: $784 \times 128 = 100{,}352$
\end{itemize}

\textbf{CNN for same image:}
\begin{itemize}
    \item Input: 28×28×1
    \item Conv layer: 32 filters, 3×3 kernel
    \item Parameters: $3 \times 3 \times 1 \times 32 + 32 = 320$ (including biases)
\end{itemize}

\textbf{300× fewer parameters!}

\clearpage
\subsubsection{Receptive Fields}

The \textbf{receptive field} of a neuron is the region of the input it "sees."

\textbf{Example:} Two 3×3 conv layers

\begin{verbatim}
Layer 1: Each neuron sees 3×3 region
Layer 2: Each neuron sees 3×3 of Layer 1
         → Sees 5×5 region of original input!
\end{verbatim}

\textbf{Receptive field grows with depth:}
\begin{itemize}
    \item 1 layer (3×3 kernel): $3 \times 3 = 9$ pixels
    \item 2 layers: $5 \times 5 = 25$ pixels
    \item 3 layers: $7 \times 7 = 49$ pixels
    \item $n$ layers: $(2n+1) \times (2n+1)$ pixels (for 3×3 kernels, stride=1)
\end{itemize}

\textbf{Formula for receptive field:}
\[
r_n = r_{n-1} + (k - 1) \cdot \prod_{i=1}^{n-1} s_i
\]

where $k$ is kernel size, $s_i$ is stride at layer $i$.

\begin{pytorchtip}[Why Receptive Fields Matter]
For a network to classify an entire image, the final layer's receptive field must cover the whole image. This determines your network depth:

\textbf{Small images (32×32):} 5-10 layers sufficient

\textbf{Large images (224×224):} 15-20+ layers needed

Use pooling or larger strides to grow receptive field faster.
\end{pytorchtip}

\subsubsection{Output Size Calculation}

\textbf{Formula for output size:}
\[
\text{Output size} = \left\lfloor \frac{\text{Input size} + 2 \times \text{Padding} - \text{Kernel size}}{\text{Stride}} \right\rfloor + 1
\]

\textbf{Examples:}

\begin{table}[h]
\centering
\begin{tabular}{cccccc}
\toprule
\textbf{Input} & \textbf{Kernel} & \textbf{Stride} & \textbf{Padding} & \textbf{Output} & \textbf{Note} \\
\midrule
28 & 3 & 1 & 0 & 26 & Shrinks by 2 \\
28 & 3 & 1 & 1 & 28 & Same size \\
28 & 3 & 2 & 0 & 13 & Half size \\
28 & 5 & 1 & 2 & 28 & Same size \\
32 & 3 & 1 & 0 & 30 & Shrinks by 2 \\
\bottomrule
\end{tabular}
\caption{Output size examples}
\end{table}

\textbf{Common padding strategies:}
\begin{itemize}
    \item \textbf{Valid (no padding):} Output shrinks
    \item \textbf{Same:} Output size = input size (for stride=1)
    \item \textbf{Full:} Maximum padding (output grows)
\end{itemize}

\subsubsection{Pooling}

Pooling \textbf{downsamples} the spatial dimensions, making the network:
\begin{itemize}
    \item More computationally efficient
    \item More translation invariant
    \item With larger receptive fields
\end{itemize}

\textbf{Max Pooling:} Take maximum value in each window

\begin{verbatim}
Input (4x4):          Max Pool 2x2, stride=2:
1  2  3  4            
2  4  6  8      →     4   8
3  6  9  12           12  16
4  8  12 16
\end{verbatim}

\textbf{Average Pooling:} Take average value in each window

\begin{verbatim}
Input (4x4):          Avg Pool 2x2, stride=2:
1  2  3  4            
2  4  6  8      →     2.25  5.25
3  6  9  12           5.25  11.25
4  8  12 16
\end{verbatim}

\textbf{When to use each:}
\begin{itemize}
    \item \textbf{Max pooling:} Most common. Preserves strong features. Good for classification.
    \item \textbf{Average pooling:} Smoother. Good for final global pooling. Sometimes better for regression.
    \item \textbf{No pooling:} Use strided convolutions instead (more learnable, modern approach)
\end{itemize}

\clearpage
\subsection{Implementation: Building CNNs in PyTorch}

\subsubsection{Conv2d: The Core Layer}

\begin{lstlisting}
import torch
import torch.nn as nn

# Basic Conv2d
conv = nn.Conv2d(
    in_channels=3,      # Input channels (e.g., RGB = 3)
    out_channels=64,    # Number of filters (output channels)
    kernel_size=3,      # 3x3 kernel
    stride=1,           # Move 1 pixel at a time
    padding=1,          # Pad with 1 pixel of zeros (same size)
    bias=True           # Include bias term
)

# Input: (batch_size, channels, height, width)
x = torch.randn(8, 3, 32, 32)  # 8 RGB images, 32x32
output = conv(x)
print(output.shape)  # torch.Size([8, 64, 32, 32])
# 64 feature maps, same spatial size due to padding=1

# Number of parameters
params = conv.weight.numel() + conv.bias.numel()
print(f"Parameters: {params}")  # 3*3*3*64 + 64 = 1792
\end{lstlisting}

\textbf{Key parameters explained:}

\begin{itemize}
    \item \textbf{in\_channels:} Depth of input (3 for RGB, 1 for grayscale)
    \item \textbf{out\_channels:} Number of filters = depth of output
    \item \textbf{kernel\_size:} Can be int (3 → 3×3) or tuple (3, 5 → 3×5)
    \item \textbf{stride:} Step size. stride=2 halves spatial dimensions
    \item \textbf{padding:} 
        \begin{itemize}
            \item int: uniform padding
            \item tuple: (pad\_height, pad\_width)
            \item 'same': auto-pad to keep size (stride=1 only)
            \item 'valid': no padding
        \end{itemize}
    \item \textbf{dilation:} Spacing between kernel elements (1 = standard, >1 = dilated)
    \item \textbf{groups:} Divide channels into groups (1 = standard, in\_channels = depthwise)
\end{itemize}

\subsubsection{Shape Calculation Practice}

\begin{lstlisting}
# Practice calculating output shapes
x = torch.randn(1, 3, 64, 64)  # Single 64x64 RGB image

# Example 1: Standard convolution
conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
out1 = conv1(x)
print(out1.shape)  # [1, 32, 64, 64] - same size

# Example 2: Strided convolution (downsampling)
conv2 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)
out2 = conv2(x)
print(out2.shape)  # [1, 32, 32, 32] - halved

# Example 3: No padding (shrinks)
conv3 = nn.Conv2d(3, 32, kernel_size=5, padding=0)
out3 = conv3(x)
print(out3.shape)  # [1, 32, 60, 60] - shrinks by 4

# Example 4: Large kernel
conv4 = nn.Conv2d(3, 32, kernel_size=7, padding=3)
out4 = conv4(x)
print(out4.shape)  # [1, 32, 64, 64] - same size

# Calculate expected output
def calc_output_size(input_size, kernel_size, stride, padding):
    return (input_size + 2*padding - kernel_size) // stride + 1

print(calc_output_size(64, 3, 1, 1))  # 64
print(calc_output_size(64, 3, 2, 1))  # 32
print(calc_output_size(64, 5, 1, 0))  # 60
\end{lstlisting}

\begin{warningbox}[Common Shape Mistakes]
\textbf{Mistake 1:} Forgetting to flatten before Linear layer
\begin{lstlisting}
# WRONG
output = conv_layers(x)  # Shape: (batch, channels, H, W)
output = fc(output)      # ERROR! Wrong shape

# CORRECT
output = conv_layers(x)
output = output.view(output.size(0), -1)  # Flatten
output = fc(output)  # OK
\end{lstlisting}

\textbf{Mistake 2:} Wrong channel order (PyTorch uses NCHW, not NHWC)
\begin{lstlisting}
# PyTorch expects: (batch, channels, height, width)
x = torch.randn(8, 32, 32, 3)  # WRONG order
x = x.permute(0, 3, 1, 2)      # Fix to (8, 3, 32, 32)
\end{lstlisting}
\end{warningbox}

\clearpage
\subsubsection{Building a Simple CNN}

\begin{lstlisting}
class SimpleCNN(nn.Module):
    """Simple CNN for MNIST (28x28 grayscale images)."""
    
    def __init__(self, num_classes=10):
        super().__init__()
        
        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        # Output: 32 x 28 x 28
        
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        # Output: 64 x 28 x 28
        
        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling
        # After pool: 64 x 14 x 14
        
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        # Output: 128 x 14 x 14
        # After pool: 128 x 7 x 7
        
        # Fully connected layers
        self.fc1 = nn.Linear(128 * 7 * 7, 256)
        self.fc2 = nn.Linear(256, num_classes)
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # Conv block 1
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        
        # Conv block 2
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        
        # Conv block 3
        x = self.relu(self.conv3(x))
        
        # Flatten
        x = x.view(x.size(0), -1)  # or x.flatten(1)
        
        # Fully connected
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# Test
model = SimpleCNN()
x = torch.randn(4, 1, 28, 28)  # 4 grayscale 28x28 images
output = model(x)
print(output.shape)  # torch.Size([4, 10])
\end{lstlisting}

\subsubsection{Modern CNN with Batch Normalization}

\begin{lstlisting}
class ModernCNN(nn.Module):
    """CNN with batch normalization and modern practices."""
    
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.features = nn.Sequential(
            # Block 1: 1 -> 32
            nn.Conv2d(1, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 28x28 -> 14x14
            
            # Block 2: 32 -> 64
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 14x14 -> 7x7
            
            # Block 3: 64 -> 128
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling
            nn.Flatten(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# Test
model = ModernCNN()
x = torch.randn(4, 1, 28, 28)
output = model(x)
print(output.shape)  # torch.Size([4, 10])
\end{lstlisting}

\begin{pytorchtip}[AdaptiveAvgPool2d vs Flatten]
\textbf{Old approach:} Fixed spatial size before FC layer
\begin{lstlisting}
x = x.view(batch_size, -1)  # Must know spatial size
fc = nn.Linear(128 * 7 * 7, num_classes)
\end{lstlisting}

\textbf{Modern approach:} Global pooling adapts to any input size
\begin{lstlisting}
x = nn.AdaptiveAvgPool2d((1, 1))(x)  # Always outputs 1x1
x = x.flatten(1)  # (batch, 128, 1, 1) -> (batch, 128)
fc = nn.Linear(128, num_classes)
\end{lstlisting}

Benefit: Same network works for different input sizes (e.g., 28×28 or 32×32).
\end{pytorchtip}

\clearpage
\subsubsection{1D Convolutions for Time Series}

\begin{lstlisting}
class TimeSeriesCNN(nn.Module):
    """1D CNN for time series or sequence data."""
    
    def __init__(self, input_channels=1, num_classes=10):
        super().__init__()
        
        self.conv1 = nn.Conv1d(
            in_channels=input_channels,
            out_channels=64,
            kernel_size=7,  # Look at 7 time steps
            padding=3
        )
        
        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)
        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)
        
        self.pool = nn.MaxPool1d(2)  # Downsample by 2
        
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(256, num_classes)
        
        self.relu = nn.ReLU()
    
    def forward(self, x):
        # x shape: (batch, channels, sequence_length)
        
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        
        x = self.relu(self.conv3(x))
        
        # Global pooling
        x = self.global_pool(x)  # (batch, 256, 1)
        x = x.squeeze(-1)        # (batch, 256)
        
        x = self.fc(x)
        return x

# Test on time series
model = TimeSeriesCNN(input_channels=1)
x = torch.randn(32, 1, 100)  # 32 samples, 1 channel, 100 time steps
output = model(x)
print(output.shape)  # torch.Size([32, 10])
\end{lstlisting}

\textbf{When to use 1D convolutions:}
\begin{itemize}
    \item Time series data (sensor readings, stock prices)
    \item Audio signals (raw waveforms)
    \item Text (character or word level)
    \item Any 1D sequence with local correlations
\end{itemize}

\subsubsection{3D Convolutions for Volumetric Data}

\begin{lstlisting}
class VolumetricCNN(nn.Module):
    """3D CNN for volumetric data (medical scans, video, 3D simulations)."""
    
    def __init__(self, in_channels=1, num_classes=2):
        super().__init__()
        
        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)
        
        self.pool = nn.MaxPool3d(2)  # Downsample all 3 dimensions
        
        self.global_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(128, num_classes)
        
        self.relu = nn.ReLU()
    
    def forward(self, x):
        # x shape: (batch, channels, depth, height, width)
        
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        
        x = self.relu(self.conv3(x))
        
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        
        x = self.fc(x)
        return x

# Test on 3D volume
model = VolumetricCNN()
x = torch.randn(4, 1, 32, 32, 32)  # 4 volumes, 32x32x32
output = model(x)
print(output.shape)  # torch.Size([4, 2])
\end{lstlisting}

\textbf{When to use 3D convolutions:}
\begin{itemize}
    \item Medical imaging (CT scans, MRI)
    \item Video (treat time as 3rd spatial dimension)
    \item 3D physical simulations
    \item Molecular structure analysis
\end{itemize}

\textbf{Warning:} 3D convolutions are very memory-intensive! Use smaller batch sizes.

\clearpage
% =============================================
% SECTION 7: CNNs - PART 2 (Advanced Topics & Exercises)
% =============================================

\subsubsection{Transposed Convolutions (Upsampling)}

Transposed convolutions (sometimes called "deconvolutions") \textbf{increase} spatial dimensions. Used in:
\begin{itemize}
    \item Autoencoders (decoder part)
    \item Generative models (GANs, VAEs)
    \item Semantic segmentation (U-Net)
    \item Super-resolution
\end{itemize}

\begin{lstlisting}
# Regular convolution (downsampling)
conv = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
x = torch.randn(1, 64, 32, 32)
out = conv(x)
print(out.shape)  # torch.Size([1, 128, 16, 16]) - halved

# Transposed convolution (upsampling)
deconv = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1,
                            output_padding=1)
out2 = deconv(out)
print(out2.shape)  # torch.Size([1, 64, 32, 32]) - doubled back
\end{lstlisting}

\textbf{Output size calculation:}
\[
\text{Output size} = (\text{Input size} - 1) \times \text{Stride} - 2 \times \text{Padding} + \text{Kernel size} + \text{Output padding}
\]

\textbf{Simple U-Net style autoencoder:}

\begin{lstlisting}
class SimpleAutoencoder(nn.Module):
    """Convolutional autoencoder for images."""
    
    def __init__(self):
        super().__init__()
        
        # Encoder (downsampling)
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, 3, stride=2, padding=1),  # 28 -> 14
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1), # 14 -> 7
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1), # 7 -> 4 (rounds up)
            nn.ReLU(),
        )
        
        # Decoder (upsampling)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, 
                              output_padding=1),  # 4 -> 7
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, 
                              output_padding=1),  # 7 -> 14
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, 
                              output_padding=1),  # 14 -> 28
            nn.Sigmoid()  # Output in [0, 1]
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# Test
model = SimpleAutoencoder()
x = torch.randn(4, 1, 28, 28)
reconstructed = model(x)
print(reconstructed.shape)  # torch.Size([4, 1, 28, 28])
\end{lstlisting}

\begin{warningbox}[Checkerboard Artifacts]
Transposed convolutions can create checkerboard artifacts (visible patterns in output).

\textbf{Solution:} Use \textbf{resize + convolution} instead:
\begin{lstlisting}
# Instead of ConvTranspose2d
nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)

# Use this (better quality)
nn.Sequential(
    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
    nn.Conv2d(64, 32, kernel_size=3, padding=1)
)
\end{lstlisting}
\end{warningbox}

\subsubsection{Dilated Convolutions}

Dilated (atrous) convolutions increase receptive field without increasing parameters or losing resolution.

\begin{lstlisting}
# Standard 3x3 convolution
conv_standard = nn.Conv2d(64, 64, kernel_size=3, padding=1)
# Receptive field: 3x3

# Dilated 3x3 convolution (dilation=2)
conv_dilated = nn.Conv2d(64, 64, kernel_size=3, padding=2, dilation=2)
# Receptive field: 5x5 (with gaps)
# Same parameters as standard 3x3!

# Effective kernel size: k + (k-1)*(d-1)
# For k=3, d=2: 3 + (3-1)*(2-1) = 5
\end{lstlisting}

\textbf{When to use dilated convolutions:}
\begin{itemize}
    \item Semantic segmentation (maintain resolution while growing receptive field)
    \item Dense prediction tasks
    \item When you need large receptive field but want to keep resolution
\end{itemize}

\clearpage
\subsubsection{Common CNN Architectures}

\textbf{VGG-style (stacking blocks):}

\begin{lstlisting}
def vgg_block(in_channels, out_channels, num_convs):
    """VGG-style block: multiple convs + pool."""
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels, 
                               kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(2, 2))
    return nn.Sequential(*layers)

class VGGStyleNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.features = nn.Sequential(
            vgg_block(3, 64, 2),    # 2 convs, 64 filters
            vgg_block(64, 128, 2),  # 2 convs, 128 filters
            vgg_block(128, 256, 3), # 3 convs, 256 filters
            vgg_block(256, 512, 3), # 3 convs, 512 filters
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
\end{lstlisting}

\textbf{Key design patterns:}
\begin{enumerate}
    \item \textbf{Double channels after pooling:} 64 → 128 → 256 → 512
    \item \textbf{Multiple convs per block:} Learn richer features before downsampling
    \item \textbf{Small kernels (3×3):} More efficient than large kernels
    \item \textbf{Batch norm after conv:} Stabilizes training
    \item \textbf{Global pooling:} More flexible than fixed FC input size
\end{enumerate}

\subsection{Implementation: Debugging CNNs}

\subsubsection{Visualizing Feature Maps}

\begin{lstlisting}
def visualize_feature_maps(model, x, layer_num=0):
    """Visualize activations from a specific layer."""
    activation = {}
    
    def get_activation(name):
        def hook(model, input, output):
            activation[name] = output.detach()
        return hook
    
    # Register hook
    layer_name = f'features.{layer_num}'
    handle = dict(model.named_modules())[layer_name].register_forward_hook(
        get_activation(layer_name))
    
    # Forward pass
    model.eval()
    with torch.no_grad():
        _ = model(x)
    
    handle.remove()
    
    # Visualize
    act = activation[layer_name].squeeze()  # Remove batch dim
    
    import matplotlib.pyplot as plt
    fig, axes = plt.subplots(4, 8, figsize=(16, 8))
    for i, ax in enumerate(axes.flat):
        if i < act.shape[0]:
            ax.imshow(act[i].cpu(), cmap='viridis')
            ax.axis('off')
    plt.tight_layout()
    plt.show()

# Usage
# model = ModernCNN()
# x = torch.randn(1, 1, 28, 28)
# visualize_feature_maps(model, x, layer_num=0)
\end{lstlisting}

\subsubsection{Common CNN Debugging Issues}

\textbf{Problem: Output shape wrong}

\begin{lstlisting}
# Debug shape through network
def debug_shapes(model, input_shape):
    """Print shapes through the network."""
    x = torch.randn(*input_shape)
    print(f"Input: {x.shape}")
    
    for name, module in model.named_children():
        x = module(x)
        print(f"{name}: {x.shape}")

# Usage
model = SimpleCNN()
debug_shapes(model, (1, 1, 28, 28))
"""
Input: torch.Size([1, 1, 28, 28])
conv1: torch.Size([1, 32, 28, 28])
pool: torch.Size([1, 32, 14, 14])
...
"""
\end{lstlisting}

\textbf{Problem: Memory issues with 3D convolutions}

\begin{lstlisting}
# Use gradient checkpointing for large models
from torch.utils.checkpoint import checkpoint

class MemoryEfficientCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv3d(1, 32, 3)
        self.conv2 = nn.Conv3d(32, 64, 3)
        # ... more layers
    
    def forward(self, x):
        # Checkpoint expensive layers
        x = checkpoint(self.conv1, x)
        x = checkpoint(self.conv2, x)
        return x
\end{lstlisting}

\textbf{Tips for reducing memory:}
\begin{itemize}
    \item Use smaller batch sizes
    \item Use mixed precision training (FP16)
    \item Reduce number of filters
    \item Use gradient checkpointing
    \item Process data in patches (for very large images)
\end{itemize}

\clearpage
\subsection{Exercises}

\begin{exercise}[7.1: Basic CNN for MNIST - $\bigstar\bigstar$]
\textbf{Goal:} Build and train your first CNN.

\begin{enumerate}
    \item Build a CNN for MNIST: 2 conv layers + 2 FC layers
    \item Use 32 and 64 filters, 3×3 kernels, ReLU, max pooling
    \item Train for 5 epochs
    \item Achieve >95\% test accuracy
    \item Visualize some predictions
\end{enumerate}

\textbf{Starter code:}
\begin{lstlisting}
from torchvision import datasets, transforms

# Load MNIST
transform = transforms.ToTensor()
train_dataset = datasets.MNIST('./data', train=True, 
                              download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

# Your CNN here
class MyCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # Your architecture
    
    def forward(self, x):
        # Your forward pass
        pass
\end{lstlisting}
\end{exercise}

\begin{exercise}[7.2: Shape Calculation Practice - $\bigstar\bigstar$]
\textbf{Goal:} Master output size calculations.

For a 32×32 input image, calculate output sizes for:
\begin{enumerate}
    \item Conv(3×3, stride=1, padding=1)
    \item Conv(5×5, stride=2, padding=2)
    \item Conv(3×3, stride=1, padding=0) followed by MaxPool(2×2)
    \item Three consecutive Conv(3×3, stride=1, padding=1) layers
    \item Conv(3×3, stride=1, padding=1, dilation=2)
\end{enumerate}

Then implement these layers and verify your calculations with actual tensors.
\end{exercise}

\begin{exercise}[7.3: 1D CNN for Time Series - $\bigstar\bigstar\bigstar$]
\textbf{Goal:} Apply convolutions to sequence data.

\begin{enumerate}
    \item Generate synthetic time series: $y = \sin(x) + 0.5\sin(3x) + \text{noise}$
    \item Create dataset: windows of 100 points predict next 10 points
    \item Build a 1D CNN: 3 conv layers with increasing filters
    \item Train and compare with a simple MLP baseline
    \item Visualize predictions on test data
\end{enumerate}

\textbf{Hint:} Use \texttt{nn.Conv1d} with kernel\_size=7 or 9.
\end{exercise}

\begin{exercise}[7.4: VGG-Style Network - $\bigstar\bigstar\bigstar$]
\textbf{Goal:} Build a deeper network with proper architecture.

\begin{enumerate}
    \item Implement a VGG-style network:
    \begin{itemize}
        \item Block 1: 2×Conv(64) + Pool
        \item Block 2: 2×Conv(128) + Pool
        \item Block 3: 3×Conv(256) + Pool
        \item Classifier: Global pooling + FC layers
    \end{itemize}
    \item Add batch normalization after each conv
    \item Train on CIFAR-10
    \item Achieve >70\% test accuracy
\end{enumerate}

\textbf{Challenge:} Add dropout and compare with/without it.
\end{exercise}

\begin{exercise}[7.5: Simple U-Net - $\bigstar\bigstar\bigstar\bigstar$]
\textbf{Goal:} Implement encoder-decoder architecture.

Build a simple U-Net for image denoising:
\begin{enumerate}
    \item Encoder: 3 conv blocks with downsampling
    \item Decoder: 3 transposed conv blocks with upsampling
    \item Add skip connections (concatenate encoder features with decoder)
    \item Train on MNIST with added Gaussian noise
    \item Visualize: noisy input → denoised output → clean target
\end{enumerate}

\textbf{Architecture:}
\begin{verbatim}
Encoder:        Decoder:
1 -> 32 -------> concat -> 32 -> 1
     |                      |
32 -> 64 -------> concat -> 64 -> 32
     |                      |
64 -> 128 -----> 128 -> 64
\end{verbatim}

\textbf{Starter code:}
\begin{lstlisting}
class SimpleUNet(nn.Module):
    def __init__(self):
        super().__init__()
        # Encoder
        self.enc1 = self.conv_block(1, 32)
        self.enc2 = self.conv_block(32, 64)
        self.enc3 = self.conv_block(64, 128)
        
        # Decoder
        self.dec3 = self.upconv_block(128, 64)
        self.dec2 = self.upconv_block(128, 32)  # 128 = 64 + 64 from skip
        self.dec1 = nn.Conv2d(64, 1, 1)  # 64 = 32 + 32 from skip
        
        self.pool = nn.MaxPool2d(2)
    
    def conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU()
        )
    
    def upconv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2),
            nn.BatchNorm2d(out_ch),
            nn.ReLU()
        )
    
    def forward(self, x):
        # Encoder with skip connections
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        
        # Decoder with skip connections
        d3 = self.dec3(e3)
        d2 = self.dec2(torch.cat([d3, e2], dim=1))  # Skip connection
        d1 = self.dec1(torch.cat([d2, e1], dim=1))  # Skip connection
        
        return torch.sigmoid(d1)
\end{lstlisting}
\end{exercise}

\begin{exercise}[7.6: 3D CNN for Volumetric Data - $\bigstar\bigstar\bigstar\bigstar$]
\textbf{Goal:} Work with 3D convolutions.

\begin{enumerate}
    \item Generate synthetic 3D data: 3D Gaussian blobs as "tumors"
    \item Build a 3D CNN classifier: 3 conv layers + global pooling + FC
    \item Handle memory constraints (small batch size, fewer filters)
    \item Achieve good classification accuracy
    \item Visualize 3D volumes and predictions (use slicing)
\end{enumerate}

\textbf{Data generation:}
\begin{lstlisting}
def generate_3d_blob(size=32):
    """Generate 3D volume with central blob."""
    volume = np.zeros((size, size, size))
    center = size // 2
    
    # Create Gaussian blob
    for i in range(size):
        for j in range(size):
            for k in range(size):
                dist = np.sqrt((i-center)**2 + (j-center)**2 + 
                              (k-center)**2)
                volume[i, j, k] = np.exp(-dist**2 / 100)
    
    return torch.FloatTensor(volume).unsqueeze(0)  # Add channel dim
\end{lstlisting}

\textbf{Warning:} Use batch\_size=2 or 4 for 3D convolutions!
\end{exercise}

\clearpage
\subsection{Key Takeaways}

\textbf{Why CNNs work:}
\begin{itemize}
    \item \textbf{Parameter sharing:} Drastically fewer parameters than MLPs
    \item \textbf{Translation invariance:} Features work regardless of position
    \item \textbf{Hierarchical learning:} Low-level → high-level features
    \item \textbf{Local connectivity:} Exploits spatial structure
\end{itemize}

\textbf{Architecture design principles:}
\begin{itemize}
    \item Start with 32-64 filters, double after each pooling
    \item Use small kernels (3×3) rather than large ones
    \item Add batch normalization for deep networks
    \item Use strided convolutions or pooling for downsampling
    \item Global pooling at the end for flexibility
\end{itemize}

\textbf{Convolution variants:}
\begin{itemize}
    \item \textbf{1D:} Time series, audio, text sequences
    \item \textbf{2D:} Images (most common)
    \item \textbf{3D:} Videos, medical scans, volumetric data
    \item \textbf{Transposed:} Upsampling in decoder/generator
    \item \textbf{Dilated:} Large receptive field without losing resolution
\end{itemize}

\textbf{Common patterns:}
\begin{itemize}
    \item \textbf{VGG-style:} Stack conv blocks, double channels after pooling
    \item \textbf{Encoder-decoder:} Downsample then upsample (autoencoders, U-Net)
    \item \textbf{Skip connections:} Connect encoder to decoder (U-Net, ResNet)
\end{itemize}

\textbf{Debugging tips:}
\begin{itemize}
    \item Always check shapes at each layer
    \item Visualize feature maps to understand what network learns
    \item Start small (few layers, few filters) then scale up
    \item Use \texttt{AdaptiveAvgPool2d} for flexibility
    \item Watch out for memory with 3D convolutions
\end{itemize}

\clearpage