% =============================================
% SECTION 15: PERFORMANCE & OPTIMIZATION
% =============================================

\section{Performance \& Optimization}

\subsection{Introduction: Making Training Faster}

Training can be slow. A model that takes 1 hour vs 10 hours per epoch makes a huge difference in development speed.

This section covers:
\begin{itemize}
    \item Mixed precision training (2× speedup)
    \item GPU optimization
    \item DataLoader efficiency
    \item Memory management
    \item Profiling tools
\end{itemize}

\subsection{Mixed Precision Training}

\textbf{Idea:} Use FP16 (half precision) instead of FP32 (full precision) for most operations.

\textbf{Benefits:}
\begin{itemize}
    \item 2-3× faster training
    \item 2× less memory (can use larger batch sizes)
    \item Minimal accuracy loss
\end{itemize}

\subsubsection{Automatic Mixed Precision (AMP)}

PyTorch provides automatic mixed precision via \texttt{torch.cuda.amp}:

\begin{lstlisting}
from torch.cuda.amp import autocast, GradScaler

model = YourModel().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Create gradient scaler
scaler = GradScaler()

for epoch in range(num_epochs):
    for x, y in train_loader:
        x, y = x.cuda(), y.cuda()
        
        optimizer.zero_grad()
        
        # Forward pass in mixed precision
        with autocast():
            output = model(x)
            loss = criterion(output, y)
        
        # Backward pass with gradient scaling
        scaler.scale(loss).backward()
        
        # Unscale gradients and step
        scaler.step(optimizer)
        scaler.update()
\end{lstlisting}

\textbf{How it works:}
\begin{enumerate}
    \item \texttt{autocast()}: Automatically chooses FP16 or FP32 for each operation
    \item \texttt{GradScaler}: Scales loss to prevent underflow in FP16 gradients
\end{enumerate}

\begin{pytorchtip}[When to Use Mixed Precision]
\textbf{Use AMP when:}
\begin{itemize}
    \item Training on modern GPUs (Volta, Turing, Ampere)
    \item You want faster training
    \item You're memory-limited
\end{itemize}

\textbf{Don't use if:}
\begin{itemize}
    \item GPU doesn't support FP16 (e.g., older GPUs)
    \item Model has numerical instability issues
    \item You need exact FP32 precision (rare)
\end{itemize}

\textbf{Typical speedup:} 1.5-3× depending on model and GPU.
\end{pytorchtip}

\subsubsection{Gradient Checkpointing}

Trade computation for memory:

\begin{lstlisting}
from torch.utils.checkpoint import checkpoint

class MemoryEfficientBlock(nn.Module):
    def __init__(self, ...):
        super().__init__()
        self.layer = ExpensiveLayer()
    
    def forward(self, x):
        # Use checkpointing for this layer
        return checkpoint(self.layer, x)

# Saves activations strategically
# Recomputes forward pass during backward (uses more time, less memory)
\end{lstlisting}

\textbf{When to use:}
\begin{itemize}
    \item Very deep models (100+ layers)
    \item Running out of GPU memory
    \item Willing to trade 20-30\% slower training for 2× memory reduction
\end{itemize}

\clearpage
\subsection{GPU Optimization}

\subsubsection{Data Transfer Optimization}

\textbf{Pin memory for faster CPU→GPU transfer:}

\begin{lstlisting}
train_loader = DataLoader(
    dataset,
    batch_size=128,
    num_workers=4,
    pin_memory=True  # Faster transfers
)

# Also call .cuda(non_blocking=True)
for x, y in train_loader:
    x = x.cuda(non_blocking=True)
    y = y.cuda(non_blocking=True)
    
    # Model forward...
\end{lstlisting}

\textbf{Keep data on GPU when possible:}

\begin{lstlisting}
# BAD: Moving data back and forth
for x, y in train_loader:
    x = x.cuda()
    output = model(x)
    output = output.cpu()  # Unnecessary transfer!
    loss = criterion(output, y.cpu())  # Another transfer!

# GOOD: Keep everything on GPU
for x, y in train_loader:
    x, y = x.cuda(), y.cuda()
    output = model(x)  # Stays on GPU
    loss = criterion(output, y)  # All on GPU
\end{lstlisting}

\subsubsection{Batch Size and GPU Utilization}

\begin{lstlisting}
import torch

# Check GPU utilization
# nvidia-smi in terminal

# If GPU utilization < 80%, try:
# 1. Increase batch size
train_loader = DataLoader(dataset, batch_size=256)  # Larger

# 2. Increase num_workers
train_loader = DataLoader(dataset, batch_size=128, num_workers=8)

# 3. Use larger model (if not memory-limited)
\end{lstlisting}

\subsubsection{Multi-GPU Training}

\textbf{DataParallel (simple but not recommended):}

\begin{lstlisting}
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs")
    model = nn.DataParallel(model)

model = model.cuda()

# Training as usual
# Batch automatically split across GPUs
\end{lstlisting}

\textbf{DistributedDataParallel (better, recommended):}

\begin{lstlisting}
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def train(rank, world_size):
    setup(rank, world_size)
    
    # Create model and move to GPU
    model = YourModel().to(rank)
    model = DDP(model, device_ids=[rank])
    
    # Create distributed sampler
    sampler = torch.utils.data.distributed.DistributedSampler(
        dataset, num_replicas=world_size, rank=rank
    )
    
    train_loader = DataLoader(dataset, batch_size=128, sampler=sampler)
    
    # Training loop as usual
    for epoch in range(num_epochs):
        sampler.set_epoch(epoch)  # Important for shuffling!
        for x, y in train_loader:
            # Training step...
            pass
    
    cleanup()

# Launch with torch.multiprocessing
import torch.multiprocessing as mp
mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)

# Or use torchrun:
# torchrun --nproc_per_node=4 train.py
\end{lstlisting}

\textbf{DDP vs DataParallel:}
\begin{itemize}
    \item DDP: Faster, more efficient, recommended
    \item DP: Simpler but slower, single-process
\end{itemize}

\clearpage
\subsection{DataLoader Optimization}

\subsubsection{Optimal Number of Workers}

\begin{lstlisting}
import time

def benchmark_dataloader(num_workers):
    """Find optimal num_workers."""
    loader = DataLoader(dataset, batch_size=128, 
                       num_workers=num_workers, pin_memory=True)
    
    start = time.time()
    for _ in loader:
        pass  # Just iterate
    elapsed = time.time() - start
    
    return elapsed

# Test different values
for num_workers in [0, 2, 4, 8, 16]:
    elapsed = benchmark_dataloader(num_workers)
    print(f"num_workers={num_workers}: {elapsed:.2f}s")

# Typical optimal: 4-8 workers
# More workers = more CPU usage but faster
# Too many workers = overhead dominates
\end{lstlisting}

\textbf{Guidelines:}
\begin{itemize}
    \item Start with \texttt{num\_workers=4}
    \item If CPU usage low: increase
    \item If CPU usage 100\%: decrease
    \item Typical range: 2-8 workers
\end{itemize}

\subsubsection{Prefetching}

PyTorch DataLoader automatically prefetches, but you can optimize:

\begin{lstlisting}
# Use persistent_workers to avoid recreating workers
train_loader = DataLoader(
    dataset,
    batch_size=128,
    num_workers=4,
    pin_memory=True,
    persistent_workers=True,  # Keep workers alive between epochs
    prefetch_factor=2  # Number of batches to prefetch per worker
)
\end{lstlisting}

\subsection{Memory Management}

\subsubsection{Monitoring Memory Usage}

\begin{lstlisting}
import torch

def print_memory_usage():
    """Print current GPU memory usage."""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        print(f"Allocated: {allocated:.2f} GB")
        print(f"Reserved: {reserved:.2f} GB")

# Check memory before and after operations
print("Before model creation:")
print_memory_usage()

model = LargeModel().cuda()

print("After model creation:")
print_memory_usage()
\end{lstlisting}

\subsubsection{Reducing Memory Usage}

\textbf{1. Use gradient accumulation instead of large batches:}

\begin{lstlisting}
# Instead of batch_size=512 (OOM)
# Use batch_size=128 with accumulation

accumulation_steps = 4
effective_batch_size = 128 * 4  # = 512

optimizer.zero_grad()

for i, (x, y) in enumerate(train_loader):
    output = model(x)
    loss = criterion(output, y)
    loss = loss / accumulation_steps  # Scale loss
    loss.backward()  # Accumulate gradients
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
\end{lstlisting}

\textbf{2. Clear cache periodically:}

\begin{lstlisting}
# If encountering OOM during long training
if epoch % 10 == 0:
    torch.cuda.empty_cache()
\end{lstlisting}

\textbf{3. Delete unnecessary variables:}

\begin{lstlisting}
# In evaluation, don't keep gradients
with torch.no_grad():
    for x, y in val_loader:
        output = model(x)
        # ...

# If you need to keep large tensors temporarily
large_tensor = compute_something()
result = process(large_tensor)
del large_tensor  # Free memory immediately
torch.cuda.empty_cache()
\end{lstlisting}

\textbf{4. Use smaller data types:}

\begin{lstlisting}
# FP32 (default)
x = torch.randn(1000, 1000)  # 4 MB

# FP16 (half)
x = torch.randn(1000, 1000, dtype=torch.float16)  # 2 MB

# For labels, use appropriate type
labels = torch.randint(0, 10, (1000,), dtype=torch.long)  # Not float!
\end{lstlisting}

\clearpage
\subsection{Profiling and Bottleneck Analysis}

\subsubsection{PyTorch Profiler}

\begin{lstlisting}
from torch.profiler import profile, ProfilerActivity

model = YourModel().cuda()

# Profile training
with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    for i, (x, y) in enumerate(train_loader):
        if i >= 10:  # Profile first 10 batches
            break
        
        x, y = x.cuda(), y.cuda()
        
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

# Print profiling results
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

# Export for Chrome tracing
prof.export_chrome_trace("trace.json")
# View at chrome://tracing
\end{lstlisting}

\subsubsection{Simple Timing}

\begin{lstlisting}
import time

# Time entire epoch
start = time.time()
for x, y in train_loader:
    # Training step...
    pass
elapsed = time.time() - start
print(f"Epoch time: {elapsed:.2f}s")

# Time individual operations
torch.cuda.synchronize()  # Wait for GPU
start = time.time()

output = model(x)

torch.cuda.synchronize()
elapsed = time.time() - start
print(f"Forward pass: {elapsed*1000:.2f}ms")
\end{lstlisting}

\subsubsection{Identify Bottlenecks}

\begin{lstlisting}
def profile_training_loop(model, train_loader, num_batches=10):
    """Profile different parts of training."""
    
    times = {
        'data_loading': 0,
        'to_gpu': 0,
        'forward': 0,
        'backward': 0,
        'optimizer': 0,
        'total': 0
    }
    
    total_start = time.time()
    data_start = time.time()
    
    for i, (x, y) in enumerate(train_loader):
        if i >= num_batches:
            break
        
        times['data_loading'] += time.time() - data_start
        
        # Transfer to GPU
        gpu_start = time.time()
        x, y = x.cuda(), y.cuda()
        torch.cuda.synchronize()
        times['to_gpu'] += time.time() - gpu_start
        
        # Forward
        forward_start = time.time()
        output = model(x)
        torch.cuda.synchronize()
        times['forward'] += time.time() - forward_start
        
        # Backward
        backward_start = time.time()
        loss = criterion(output, y)
        loss.backward()
        torch.cuda.synchronize()
        times['backward'] += time.time() - backward_start
        
        # Optimizer
        opt_start = time.time()
        optimizer.step()
        optimizer.zero_grad()
        torch.cuda.synchronize()
        times['optimizer'] += time.time() - opt_start
        
        data_start = time.time()
    
    times['total'] = time.time() - total_start
    
    # Print breakdown
    print("Time breakdown:")
    for key, value in times.items():
        if key != 'total':
            pct = (value / times['total']) * 100
            print(f"  {key}: {value:.3f}s ({pct:.1f}%)")
        else:
            print(f"  {key}: {value:.3f}s")

# Usage
profile_training_loop(model, train_loader)
"""
Time breakdown:
  data_loading: 0.421s (25.3%)
  to_gpu: 0.089s (5.3%)
  forward: 0.567s (34.1%)
  backward: 0.423s (25.4%)
  optimizer: 0.165s (9.9%)
  total: 1.665s
"""

# If data_loading is high: increase num_workers
# If forward/backward is high: model is slow (expected)
# If to_gpu is high: use pin_memory, non_blocking transfers
\end{lstlisting}

\clearpage
\subsection{Production Deployment}

\subsubsection{Model Export}

\textbf{TorchScript (JIT compilation):}

\begin{lstlisting}
# Option 1: Tracing
model = YourModel()
model.eval()

example_input = torch.randn(1, 3, 224, 224)
traced_model = torch.jit.trace(model, example_input)

# Save
traced_model.save("model_traced.pt")

# Load and use
loaded_model = torch.jit.load("model_traced.pt")
output = loaded_model(example_input)
\end{lstlisting}

\begin{lstlisting}
# Option 2: Scripting (handles control flow)
scripted_model = torch.jit.script(model)
scripted_model.save("model_scripted.pt")
\end{lstlisting}

\textbf{ONNX (cross-framework):}

\begin{lstlisting}
import torch.onnx

model = YourModel()
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)

torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)

# Can now use in other frameworks (TensorFlow, etc.)
\end{lstlisting}

\subsubsection{Inference Optimization}

\begin{lstlisting}
# 1. Set to eval mode
model.eval()

# 2. Disable gradient computation
with torch.no_grad():
    output = model(input)

# 3. Use FP16 if possible
with torch.cuda.amp.autocast():
    output = model(input)

# 4. Batch inference
# Process multiple inputs together
batch = torch.stack([input1, input2, input3])
outputs = model(batch)

# 5. Use torch.jit for production
model = torch.jit.load("model.pt")
\end{lstlisting}

\subsection{Key Takeaways}

\textbf{Mixed precision training:}
\begin{itemize}
    \item Use \texttt{torch.cuda.amp} for 2× speedup
    \item Minimal code changes required
    \item Works on modern GPUs (Volta+)
    \item Typical speedup: 1.5-3×
\end{itemize}

\textbf{GPU optimization:}
\begin{itemize}
    \item Keep data on GPU (avoid CPU↔GPU transfers)
    \item Use \texttt{pin\_memory=True} and \texttt{non\_blocking=True}
    \item Maximize batch size (within memory limits)
    \item Use DistributedDataParallel for multi-GPU
\end{itemize}

\textbf{DataLoader optimization:}
\begin{itemize}
    \item Start with \texttt{num\_workers=4}
    \item Use \texttt{persistent\_workers=True}
    \item Benchmark different worker counts
    \item Typical optimal: 4-8 workers
\end{itemize}

\textbf{Memory management:}
\begin{itemize}
    \item Use gradient accumulation for large effective batch sizes
    \item Clear cache periodically if needed
    \item Delete unnecessary variables
    \item Use gradient checkpointing for very deep models
\end{itemize}

\textbf{Profiling:}
\begin{itemize}
    \item Use PyTorch Profiler for detailed analysis
    \item Identify bottlenecks (data loading, forward, backward)
    \item Focus optimization efforts on slowest parts
    \item Profile regularly during development
\end{itemize}

\textbf{Production deployment:}
\begin{itemize}
    \item Use TorchScript for inference
    \item Export to ONNX for cross-framework compatibility
    \item Always use \texttt{model.eval()} and \texttt{torch.no\_grad()}
    \item Batch inference when possible
    \item Consider FP16 inference
\end{itemize}

\textbf{Common performance issues:}
\begin{itemize}
    \item Data loading too slow → increase \texttt{num\_workers}
    \item GPU underutilized → increase batch size or model size
    \item Out of memory → reduce batch size, use gradient accumulation, or AMP
    \item Training too slow → use mixed precision, profile bottlenecks
\end{itemize}

\clearpage