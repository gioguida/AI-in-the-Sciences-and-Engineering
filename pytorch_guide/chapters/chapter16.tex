% =============================================
% PART IV: INTEGRATIVE CHALLENGES
% =============================================

\part{Integrative Challenges}

% =============================================
% SECTION 16: GRAND CHALLENGES
% =============================================

\section{Grand Challenges}

\subsection{Introduction: Putting It All Together}

You've learned the fundamentals. Now it's time to combine everything into complete, real-world projects.

These challenges integrate multiple concepts:
\begin{itemize}
    \item Multiple architecture types (CNNs, RNNs, Transformers)
    \item Proper training pipelines (data loading, optimization, monitoring)
    \item Best practices (regularization, debugging, performance)
    \item Scientific computing applications
\end{itemize}

\textbf{How to approach these challenges:}
\begin{enumerate}
    \item Start simple (baseline model)
    \item Get it working (overfit small data)
    \item Make it good (full dataset + regularization)
    \item Make it great (hyperparameter tuning + tricks)
    \item Document everything (what worked, what didn't)
\end{enumerate}

\subsection{Challenge 1: Time Series Forecasting System}

\textbf{Difficulty:} $\bigstar\bigstar\bigstar$

\textbf{Goal:} Build a complete forecasting system for multivariate time series.

\textbf{Dataset:} Use real data (weather, stock prices, sensor readings) or generate complex synthetic data.

\textbf{Requirements:}

\textbf{1. Data Pipeline}
\begin{itemize}
    \item Load and preprocess time series data
    \item Create sliding windows (past 100 steps → predict next 10)
    \item Handle missing values
    \item Normalize using training statistics
    \item Split into train/val/test (temporal split, not random!)
\end{itemize}

\textbf{2. Model Architecture}

Implement and compare three approaches:
\begin{itemize}
    \item \textbf{LSTM-based:} Stacked LSTM with attention
    \item \textbf{CNN-based:} 1D CNN with residual connections
    \item \textbf{Transformer-based:} Encoder-only Transformer
\end{itemize}

\textbf{3. Training}
\begin{itemize}
    \item Proper learning rate schedule (warmup + decay)
    \item Early stopping on validation set
    \item Gradient clipping (essential for RNNs)
    \item Mixed precision training
    \item TensorBoard logging
\end{itemize}

\textbf{4. Evaluation}
\begin{itemize}
    \item Multiple metrics: MSE, MAE, MAPE
    \item One-step ahead vs multi-step ahead prediction
    \item Visualization: predicted vs actual
    \item Confidence intervals (optional)
\end{itemize}

\textbf{5. Analysis}
\begin{itemize}
    \item Which architecture works best?
    \item How far ahead can you predict accurately?
    \item What patterns does the model learn?
    \item Where does it fail?
\end{itemize}

\textbf{Starter template:}
\begin{lstlisting}
class TimeSeriesDataset(Dataset):
    def __init__(self, data, window_size, forecast_horizon):
        # Your implementation
        pass
    
    def __len__(self):
        return len(self.data) - self.window_size - self.forecast_horizon + 1
    
    def __getitem__(self, idx):
        # Return (input_window, target_window)
        pass

class LSTMForecaster(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, forecast_horizon):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True)
        self.fc = nn.Linear(hidden_size, forecast_horizon)
    
    def forward(self, x):
        # LSTM encoding
        output, (h_n, c_n) = self.lstm(x)
        # Predict from final hidden state
        prediction = self.fc(h_n[-1])
        return prediction

# Your training loop with all best practices
\end{lstlisting}

\textbf{Bonus challenges:}
\begin{itemize}
    \item Add attention mechanism to LSTM
    \item Implement probabilistic forecasting (predict distribution, not point estimate)
    \item Handle multiple related time series (multivariate)
    \item Deploy as REST API
\end{itemize}

\clearpage
\subsection{Challenge 2: Hybrid Vision-Language Model}

\textbf{Difficulty:} $\bigstar\bigstar\bigstar\bigstar$

\textbf{Goal:} Build a model that combines visual and textual information.

\textbf{Task:} Image captioning or visual question answering.

\textbf{Requirements:}

\textbf{1. Architecture}

Implement encoder-decoder with:
\begin{itemize}
    \item \textbf{Image encoder:} ResNet or Vision Transformer
    \item \textbf{Text decoder:} LSTM or Transformer decoder
    \item \textbf{Cross-modal attention:} Decoder attends to image features
\end{itemize}

\begin{lstlisting}
class ImageCaptioningModel(nn.Module):
    def __init__(self, vocab_size, embed_dim=512, hidden_dim=512):
        super().__init__()
        
        # Image encoder (pretrained ResNet)
        resnet = models.resnet50(pretrained=True)
        # Remove final classification layer
        self.encoder = nn.Sequential(*list(resnet.children())[:-2])
        
        # Freeze encoder (transfer learning)
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        # Project image features
        self.image_proj = nn.Linear(2048, embed_dim)
        
        # Text decoder
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, 
                           batch_first=True)
        
        # Attention mechanism
        self.attention = nn.MultiheadAttention(embed_dim, num_heads=8)
        
        # Output projection
        self.fc_out = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, images, captions):
        # Encode images: (batch, 2048, 7, 7)
        img_features = self.encoder(images)
        
        # Reshape: (batch, 49, 2048)
        batch_size = img_features.size(0)
        img_features = img_features.view(batch_size, 2048, -1)
        img_features = img_features.transpose(1, 2)
        
        # Project: (batch, 49, embed_dim)
        img_features = self.image_proj(img_features)
        
        # Embed captions
        caption_embeds = self.embedding(captions)
        
        # LSTM decoding with attention
        lstm_out, _ = self.lstm(caption_embeds)
        
        # Attend to image features
        attn_out, _ = self.attention(lstm_out, img_features, img_features)
        
        # Combine and project
        combined = lstm_out + attn_out
        output = self.fc_out(combined)
        
        return output
\end{lstlisting}

\textbf{2. Data Preparation}
\begin{itemize}
    \item Use COCO dataset or similar
    \item Build vocabulary from captions
    \item Tokenize captions
    \item Implement proper data augmentation for images
    \item Handle variable-length captions (padding + masking)
\end{itemize}

\textbf{3. Training}
\begin{itemize}
    \item Teacher forcing during training
    \item Proper cross-entropy loss (ignore padding tokens)
    \item Learning rate warmup + decay
    \item Mixed precision training
    \item Gradient accumulation (if memory limited)
\end{itemize}

\textbf{4. Generation}

Implement multiple decoding strategies:
\begin{lstlisting}
def generate_caption(model, image, vocab, max_len=20, 
                    strategy='greedy', temperature=1.0, beam_width=5):
    """
    Generate caption from image.
    
    Args:
        strategy: 'greedy', 'sampling', or 'beam_search'
    """
    model.eval()
    
    # Encode image
    with torch.no_grad():
        img_features = model.encode_image(image)
    
    # Start token
    caption = [vocab['<start>']]
    
    if strategy == 'greedy':
        # Greedy decoding (take argmax at each step)
        for _ in range(max_len):
            logits = model.decode_step(img_features, caption)
            next_word = logits.argmax()
            caption.append(next_word.item())
            if next_word == vocab['<end>']:
                break
    
    elif strategy == 'sampling':
        # Sample from distribution
        for _ in range(max_len):
            logits = model.decode_step(img_features, caption)
            logits = logits / temperature
            probs = F.softmax(logits, dim=-1)
            next_word = torch.multinomial(probs, 1)
            caption.append(next_word.item())
            if next_word == vocab['<end>']:
                break
    
    elif strategy == 'beam_search':
        # Beam search (keep top-k candidates)
        # Your implementation
        pass
    
    return caption
\end{lstlisting}

\textbf{5. Evaluation}
\begin{itemize}
    \item BLEU score (text similarity metric)
    \item Qualitative: Visual inspection of generated captions
    \item Error analysis: Where does model fail?
    \item Attention visualization: What does model look at?
\end{itemize}

\textbf{Bonus challenges:}
\begin{itemize}
    \item Implement beam search decoding
    \item Add image-text contrastive learning (CLIP-style)
    \item Fine-tune encoder (not just freeze)
    \item Add copy mechanism for rare words
\end{itemize}

\clearpage
\subsection{Challenge 3: Denoising Scientific Data}

\textbf{Difficulty:} $\bigstar\bigstar\bigstar$

\textbf{Goal:} Build a U-Net style architecture to denoise 2D scientific data.

\textbf{Application:} Medical images, microscopy, sensor data, astronomical images.

\textbf{Requirements:}

\textbf{1. Data Generation}

Create noisy data:
\begin{lstlisting}
def generate_noisy_data(clean_images, noise_level=0.1):
    """Add Gaussian noise to clean images."""
    noise = torch.randn_like(clean_images) * noise_level
    noisy = clean_images + noise
    return noisy, clean_images

# Use real dataset (medical images, etc.) or synthetic
\end{lstlisting}

\textbf{2. U-Net Architecture}

Implement complete U-Net with:
\begin{itemize}
    \item Encoder: Downsampling path with residual blocks
    \item Decoder: Upsampling path with skip connections
    \item Attention gates (optional but recommended)
    \item Batch normalization throughout
\end{itemize}

\begin{lstlisting}
class UNet(nn.Module):
    """U-Net for image denoising/reconstruction."""
    
    def __init__(self, in_channels=1, out_channels=1):
        super().__init__()
        
        # Encoder
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        
        # Bottleneck
        self.bottleneck = self.conv_block(512, 1024)
        
        # Decoder with skip connections
        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = self.conv_block(1024, 512)  # 1024 = 512 + 512 from skip
        
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = self.conv_block(512, 256)
        
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = self.conv_block(256, 128)
        
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = self.conv_block(128, 64)
        
        # Output
        self.out = nn.Conv2d(64, out_channels, 1)
        
        self.pool = nn.MaxPool2d(2, 2)
    
    def conv_block(self, in_ch, out_ch):
        """Double convolution block."""
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        
        # Bottleneck
        bottleneck = self.bottleneck(self.pool(enc4))
        
        # Decoder with skip connections
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat([dec4, enc4], dim=1)
        dec4 = self.dec4(dec4)
        
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat([dec3, enc3], dim=1)
        dec3 = self.dec3(dec3)
        
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.out(dec1)
\end{lstlisting}

\textbf{3. Training}
\begin{itemize}
    \item Loss function: MSE + Perceptual loss (optional)
    \item Data augmentation: Random flips, rotations
    \item Learning rate schedule
    \item Monitor both MSE and PSNR metrics
\end{itemize}

\textbf{4. Evaluation}
\begin{lstlisting}
def calculate_psnr(img1, img2):
    """Calculate Peak Signal-to-Noise Ratio."""
    mse = torch.mean((img1 - img2) ** 2)
    if mse == 0:
        return float('inf')
    max_pixel = 1.0
    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))
    return psnr.item()

def calculate_ssim(img1, img2):
    """Structural Similarity Index (use skimage or pytorch)."""
    from skimage.metrics import structural_similarity
    return structural_similarity(
        img1.cpu().numpy(), img2.cpu().numpy(),
        data_range=1.0
    )

# Evaluate
model.eval()
psnrs = []
ssims = []

with torch.no_grad():
    for noisy, clean in test_loader:
        denoised = model(noisy)
        
        for i in range(len(noisy)):
            psnr = calculate_psnr(denoised[i], clean[i])
            ssim = calculate_ssim(denoised[i], clean[i])
            psnrs.append(psnr)
            ssims.append(ssim)

print(f"Average PSNR: {np.mean(psnrs):.2f} dB")
print(f"Average SSIM: {np.mean(ssims):.4f}")
\end{lstlisting}

\textbf{5. Visualization}
\begin{lstlisting}
def visualize_denoising(model, noisy_images, clean_images):
    """Visualize denoising results."""
    model.eval()
    
    with torch.no_grad():
        denoised = model(noisy_images)
    
    fig, axes = plt.subplots(len(noisy_images), 3, figsize=(12, 4*len(noisy_images)))
    
    for i in range(len(noisy_images)):
        # Noisy
        axes[i, 0].imshow(noisy_images[i].squeeze().cpu(), cmap='gray')
        axes[i, 0].set_title('Noisy')
        axes[i, 0].axis('off')
        
        # Denoised
        axes[i, 1].imshow(denoised[i].squeeze().cpu(), cmap='gray')
        axes[i, 1].set_title('Denoised')
        axes[i, 1].axis('off')
        
        # Clean
        axes[i, 2].imshow(clean_images[i].squeeze().cpu(), cmap='gray')
        axes[i, 2].set_title('Clean (Target)')
        axes[i, 2].axis('off')
    
    plt.tight_layout()
    plt.show()
\end{lstlisting}

\textbf{Bonus challenges:}
\begin{itemize}
    \item Add attention gates between encoder and decoder
    \item Implement progressive training (start with low resolution)
    \item Handle different noise types (Gaussian, salt-and-pepper, Poisson)
    \item Add perceptual loss using pretrained VGG
\end{itemize}

\clearpage
\subsection{Challenge 4: Custom Transformer for Sequence Tasks}

\textbf{Difficulty:} $\bigstar\bigstar\bigstar\bigstar$

\textbf{Goal:} Build a complete Transformer from scratch for sequence classification or generation.

\textbf{Task:} Text classification, sequence generation, or time series classification.

\textbf{Requirements:}

\textbf{1. Complete Transformer Implementation}

Build all components from scratch (no \texttt{nn.Transformer}):
\begin{itemize}
    \item Multi-head attention
    \item Position-wise feed-forward
    \item Positional encoding
    \item Layer normalization
    \item Encoder and/or decoder blocks
\end{itemize}

\textbf{2. Training Infrastructure}
\begin{itemize}
    \item Warmup learning rate schedule
    \item Gradient clipping
    \item Label smoothing
    \item Mixed precision training
    \item Checkpointing and recovery
\end{itemize}

\textbf{3. Advanced Features}

Implement at least two of:
\begin{itemize}
    \item Learning rate warmup + cosine annealing
    \item Custom learning rate schedule
    \item Gradient accumulation
    \item Model ensemble
    \item Knowledge distillation
\end{itemize}

\textbf{4. Comprehensive Evaluation}
\begin{itemize}
    \item Validation metrics
    \item Attention visualization
    \item Error analysis
    \item Compare with baseline (LSTM, simple MLP)
    \item Ablation studies (remove components, measure impact)
\end{itemize}

\textbf{5. Analysis}

Answer these questions:
\begin{itemize}
    \item How many heads are optimal?
    \item What does each attention head learn?
    \item How deep should the model be?
    \item What's the effect of positional encoding?
    \item Where does the model fail?
\end{itemize}

\clearpage
\subsection{Challenge 5: End-to-End ML Pipeline}

\textbf{Difficulty:} $\bigstar\bigstar\bigstar\bigstar$

\textbf{Goal:} Build a production-ready machine learning system.

\textbf{Task:} Choose your own (classification, regression, generation).

\textbf{Requirements:}

\textbf{1. Complete Pipeline}

\begin{verbatim}
project/
├── data/
│   ├── raw/              # Raw data
│   ├── processed/        # Preprocessed data
│   └── preprocessing.py  # Data processing scripts
├── models/
│   ├── architectures.py  # Model definitions
│   └── pretrained/       # Saved models
├── training/
│   ├── train.py          # Training script
│   ├── evaluate.py       # Evaluation script
│   └── utils.py          # Training utilities
├── configs/
│   ├── base_config.yaml  # Base configuration
│   └── experiment_*.yaml # Experiment configs
├── notebooks/
│   ├── exploration.ipynb # Data exploration
│   └── analysis.ipynb    # Results analysis
├── tests/
│   └── test_*.py         # Unit tests
├── requirements.txt
├── README.md
└── main.py               # Entry point
\end{verbatim}

\textbf{2. Code Quality}
\begin{itemize}
    \item Type hints
    \item Docstrings for all functions
    \item Unit tests for critical functions
    \item Configuration via YAML
    \item Logging (not just print statements)
    \item Error handling
\end{itemize}

\textbf{3. Experiment Tracking}
\begin{itemize}
    \item TensorBoard or Weights \& Biases
    \item Log hyperparameters
    \item Log metrics over time
    \item Save model checkpoints
    \item Version control (Git)
\end{itemize}

\textbf{4. Reproducibility}
\begin{itemize}
    \item Set random seeds
    \item Save exact hyperparameters
    \item Save training logs
    \item Document environment (requirements.txt)
    \item Save data preprocessing steps
\end{itemize}

\textbf{5. Deployment}
\begin{itemize}
    \item Export model (TorchScript or ONNX)
    \item Create inference script
    \item Docker container (optional)
    \item REST API using FastAPI (optional)
    \item Documentation for usage
\end{itemize}

\textbf{Evaluation criteria:}
\begin{itemize}
    \item Model performance
    \item Code quality and organization
    \item Reproducibility
    \item Documentation
    \item Completeness of pipeline
\end{itemize}

\subsection{Evaluation and Next Steps}

\textbf{How to know you've succeeded:}

\begin{enumerate}
    \item \textbf{It works:} Model trains without errors
    \item \textbf{It learns:} Loss decreases, metrics improve
    \item \textbf{It generalizes:} Validation performance is good
    \item \textbf{You understand it:} Can explain why it works
    \item \textbf{It's reproducible:} Others can replicate your results
\end{enumerate}

\textbf{Beyond this guide:}

\textbf{Keep learning:}
\begin{itemize}
    \item Read recent papers (arXiv, conferences)
    \item Implement papers from scratch
    \item Contribute to open source projects
    \item Join competitions (Kaggle, etc.)
    \item Build projects in your domain
\end{itemize}

\textbf{Advanced topics to explore:}
\begin{itemize}
    \item Self-supervised learning
    \item Meta-learning
    \item Neural architecture search
    \item Efficient networks (quantization, pruning)
    \item Diffusion models
    \item Large language models
    \item Multimodal learning
    \item Reinforcement learning
\end{itemize}

\textbf{Stay updated:}
\begin{itemize}
    \item Follow PyTorch blog and releases
    \item Join ML communities (Reddit, Discord, forums)
    \item Attend conferences (NeurIPS, ICML, ICLR)
    \item Read technical blogs
    \item Experiment with new techniques
\end{itemize}

\subsection{Final Thoughts}

You now have a comprehensive toolkit for deep learning with PyTorch. You've learned:

\begin{itemize}
    \item \textbf{Foundations:} Tensors, autograd, modules, training loops, data loading
    \item \textbf{Architectures:} MLPs, CNNs, ResNets, RNNs, Transformers
    \item \textbf{Techniques:} Normalization, regularization, optimization
    \item \textbf{Practical skills:} Debugging, training best practices, performance optimization
\end{itemize}

\textbf{Remember:}
\begin{itemize}
    \item Start simple, add complexity gradually
    \item Always check if your model can overfit small data
    \item Visualization and monitoring are essential
    \item Good engineering matters as much as good models
    \item Deep learning is iterative—experiment and learn
\end{itemize}

The best way to master these concepts is to \textbf{build things}. Pick a challenge that excites you and start coding!

Good luck, and happy building!

\clearpage
\end{lstlisting}
\end{document}